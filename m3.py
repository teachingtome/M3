# -*- coding: utf-8 -*-
"""m3

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pI3meaioqxhAYT8IhwN08ROJCQs6_fGG
"""

# Import packages
import graphviz
import math
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from scipy.optimize import curve_fit, fsolve
from scipy.signal import find_peaks
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.feature_selection import SelectFromModel
from sklearn.inspection import partial_dependence, permutation_importance, PartialDependenceDisplay
from sklearn.linear_model import LinearRegression
from sklearn import metrics
from sklearn.metrics import accuracy_score, r2_score
from sklearn.model_selection import train_test_split
from sklearn.tree import export_graphviz
from scipy.stats import linregress







# Import data and remove the last 12 rows, which have holes in them
df = pd.read_csv("ebikerff.csv")
df = df.drop(df.index[-12:])

# The independent variables and dependent variable. Is good is based on the sales increases from the previous year must be > 20% for it to be labeled as 1.
target_list = ['Number of cars','Number of bikes','Environmental perception','Disposible income','Price of gas','Price of lithium ion battery','Degrees Celsius','Rate of obesity','EPI','Number of Google searches']
X = df[target_list]
y = df["is good"]
X = X.astype('float')
y = y.astype('float')


# Split into training and testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=245)


# Train the random forest model
clf = RandomForestClassifier(n_estimators=100, random_state=14)
clf.fit(X_train, y_train)

# Visualize the forest
dot_data = export_graphviz(clf.estimators_[0], out_file=None, 
                          feature_names=target_list,  
                          class_names=[str(i) for i in clf.classes_],  
                          filled=True, rounded=True,  
                          special_characters=True)  
graph = graphviz.Source(dot_data)  
graph.render('Decision_Tree')

# Predict the class for each X value
y_pred = clf.predict(X_test)


# Print the accuracy of the model
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)


# Determine the feature importances and display them
importance = clf.feature_importances_
feat_importances = pd.Series(clf.feature_importances_, index=X.columns)
feat_importances.nlargest(20).plot(kind='barh')
plt.xlabel("Random Forest Feature Importance")
feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(target_list, importance)]

# Print the error values
print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))
print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))
print("Data Size:", df.shape)
plt.title('E-bikes')

#Annotation
for index, value in enumerate(feat_importances.nlargest(20)):
  plt.text(value, index, str(round(value, 2)))
  
plt.show()



def dispLin(col):
  df = pd.read_csv('ebike.csv')
  # Single out one IV and the DV
  df = df.dropna(subset=[col])
  x = df['US']
  y = df[col]
  # The columns needing linear regression
  if col == 'Number of cars' or col == 'Number of bikes' or col == 'Number of Google searches' or col == 'Disposible income' or col == 'Price of electricity' or col == 'Rate of obesity':
    # Linear regression values
    slope, intercept, r_value, p_value, std_err = linregress(x, y)

    # Create a scatter plot
    plt.scatter(x, y)

    # Create a line plot with the linear regression line
    plt.plot(x, slope*x + intercept, color='red')

    # Add labels
    plt.xlabel('Year')
    plt.ylabel(col)
    plt.title(col + " based on year")

    # Print the needed values, equation, slope, intercept, and predicted values
    plt.show()
    print("Equation: y= ", slope,"x +",intercept)
    print("Slope:", slope)
    print("Intercept:", intercept)
    y_2028 = slope * 2028 + intercept
    y_2025 = slope * 2025 + intercept
    print("Predicted value for 2028: ", y_2028)
    print("Predicted value for 2025: ", y_2025)
  
  # The columns needing sinusoid regression
  elif col == 'Environmental perception' or col == 'Degrees Celsius' or col == 'CPI  transportation' or col == 'EPI' or col == 'Price of gas':
    
    # Sinusoid function
    def sinusoid(x, a, b, c, d):
      return a * np.sin(b * x + c) + d

    # Fit the sinusoidal function to the data
    popt, pcov = curve_fit(sinusoid, x, y)

    # Print the coefficients of the parameters
    print("Coefficients: a = {:.2f}, b = {:.2f}, c = {:.2f}, d = {:.2f}".format(*popt))

    #Label and display values and graph
    plt.xlabel('Year')
    plt.ylabel(col)
    plt.title(col + " based on year")
    plt.scatter(x, y, label='Original Data')
    plt.plot(x, sinusoid(x, *popt), 'r-', label='Fitted Curve')
    plt.legend()
    plt.show()

    x_new = np.array([2025, 2028])
    y_new = sinusoid(x_new, *popt)
    print("Predicted values at 2025 and 2028: ", y_new)

  # The columns needing inverse regression
  elif col == 'Price of lithium ion battery':

    # Inverse function
    def inverse(x, a, b, c):
      return a / (b + x**(3/2)) + c

    # Fit the inverse function to the data
    popt, pcov = curve_fit(inverse, x, y, maxfev = 10000)

    # Print the coefficients of the parameters
    print("Optimized values: a = {:.2f}, b = {:.2f}, c = {:.2f}".format(*popt))

    # Label and display values and graph
    plt.xlabel('Year')
    plt.ylabel(col)
    plt.title(col + " based on year")
    plt.scatter(x, y, label='Original Data')
    plt.plot(x, inverse(x, *popt), 'r-', label='Fitted Curve')
    plt.legend()
    plt.show()

    x_new = np.array([2025, 2028])
    y_new = inverse(x_new, *popt)
    print("Predicted values at 2025 and 2028: ", y_new)

# Go through each column
df = pd.read_csv('ebike.csv')
for column_name in df.columns:
  if column_name != "US":   
    print(column_name)
    dispLin(column_name)

# Similar function for the USA csv
def dispLinUSA(col):
  df = pd.read_csv('ebikeUSA.csv')
  df = df.dropna(subset=[col])
  x = df['US']
  y = df[col]

  # Linear regression
  slope, intercept, r_value, p_value, std_err = linregress(x, y)

  # Create a scatter plot
  plt.scatter(x, y)

  # Create a line plot with the linear regression line
  plt.plot(x, slope*x + intercept, color='red')

  # Add labels
  plt.xlabel('Year')
  plt.ylabel(col)
  plt.title(col + " based on year")

  # Display needed values
  plt.show()
  print("Equation: y= ", slope,"x +",intercept)
  print("Slope:", slope)
  print("Intercept:", intercept)
  for i in range(2028,2022,-1):
    ypred = slope * i + intercept
    print("Predicted value for ",i,': ', ypred)

# Same basically
df = pd.read_csv('ebikeUSA.csv')
for column_name in df.columns:
  if column_name != "US" and not df[column_name].isnull().all():   
    print(column_name)
    dispLinUSA(column_name)

import numpy as np

# Input data of indexes to determine the coefficients for computing the final index
x1 = np.array([6.108,5.925,5.610,5.189,5.624,5.408,4.875,4.083,3.982,4.656,3.876], dtype='float64')
x2 = np.array([2.936,2.292,1.615,1.117,0.364,0.520,0.291,0.170,0.142,0.092,0.056], dtype='float64')
x3 = np.array([1.197,0.608,0.414,0.611,0.716,0.767,0.873,1.302,2.743,3.066,3.343], dtype='float64')
y = np.array([2.78,2.26,1.25,1.29,1.13,0.81,0.47,0.41,0.61,0.50,0.22], dtype='float64')

# The loss function we are using
def loss(w):
    y_pred = w[0] * x1 + w[1] * x2 + w[2] * x3 + w[3]
    return np.mean((y - y_pred) ** 2).astype(y.dtype)

# The gradient function
def gradient(w):
    y_pred = w[0] * x1 + w[1] * x2 + w[2] * x3 + w[3]
    dw1 = -2 * np.mean(x1 * (y - y_pred))
    dw2 = -2 * np.mean(x2 * (y - y_pred))
    dw3 = -2 * np.mean(x3 * (y - y_pred))
    dw4 = -2 * np.mean(y - y_pred)
    return np.array([dw1, dw2, dw3, dw4], dtype=w.dtype)

# Gradient descent function
def gradient_descent(w_start, learning_rate, num_iterations):
    w = w_start
    losses = []
    for i in range(num_iterations):
        dw = gradient(w)
        w -= learning_rate * dw
        losses.append(loss(w))
    return w, losses

# Parameters
w_start = np.array([0, 0, 0, 0], dtype='float64')
learning_rate = 0.01
num_iterations = 1000

#Running and outputting the optimal coefficients
w_opt, losses = gradient_descent(w_start, learning_rate, num_iterations)
print("Optimized values: w1 = {:.2f}, w2 = {:.2f}, w3 = {:.2f}, w4 = {:.2f}".format(w_opt[0], w_opt[1], w_opt[2], w_opt[3]))
print("Final loss: {:.2f}".format(loss(w_opt)))
print("Optimized values: w1 = {:.2f}, w2 = {:.2f}, w3 = {:.2f}, w4 = {:.2f}".format(w_opt[0], w_opt[1], w_opt[2], w_opt[3]))